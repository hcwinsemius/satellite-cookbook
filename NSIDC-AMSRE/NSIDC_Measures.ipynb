{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download, slice, concatenate and store data from the National Snow and Ice Data Center ##\n",
    "This recipe demonstrates how you can prepare a nicely formatted yearly organised set of properly formatted NetCDF files from the NSIDC database Measures. The recipe is tested on AMSR-E data (25Km) and set up to allow for variations in frequency, Ascending/Descending orbits and Horizontal / VErtical polarization. These settings are used to construct download urls which are then retrieved using wget behind the scenes. If you are using windows, please retrieve a wget .exe for your system from https://eternallybored.org/misc/wget/\n",
    "\n",
    "Information about datasets that may also be retrieved (but may require different settings and projection information) can be found on the nsidc pages for each product for instance http://nsidc.org/data/nsidc-0630\n",
    "\n",
    "To make the recipe work for tour product of interest, a reconstruction of a proper url template is needed. Have a look in the code below to adapt that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import os\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import pyproj\n",
    "import osr\n",
    "import datetime\n",
    "import subprocess\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# import our own small lib\n",
    "import nsidc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, construct a bunch of settings for your specific data of interest. The idea is to construct urls in a loop by filling in a template, and retrieve these urls. Then read them, manipulate them and store them separately. \n",
    "- supply your own nasa earthdata username and password. \n",
    "- fill in the required bits and pieces to retrieve the right data for your case (Change template accordingly)\n",
    "In the case below, we vary the url string accoding to frequency, polarization, orbit and time span. This may be different for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://n5eil01u.ecs.nsidc.org/MEASURES/NSIDC-0630.001/'\n",
    "url_folder = '{:s}/'\n",
    "url_template = url_base + url_folder + 'NSIDC-0630-EASE2_T25km-AQUA_AMSRE-{:s}-{:s}{:s}-{:s}-GRD-RSS-v1.3.nc'\n",
    "download_template = 'wget --http-user={:s} --http-password={:s} --load-cookies mycookies.txt --save-cookies mycookies.txt --keep-session-cookies --no-check-certificate --auth-no-challenge -r --reject \"index.html*\" -np -e robots=off {:s}'\n",
    "\n",
    "fn_out_template = 'NSIDC-0630-EASE2_T25km-AQUA-AMSRE-{:s}{:s}-{:s}_{:04d}.nc'\n",
    "out_path = os.path.abspath('netcdf')\n",
    "if not(os.path.isdir(out_path)):\n",
    "    # prepare the output path\n",
    "    os.makedirs(out_path)\n",
    "# username = 'FILLINYOURUSERNAME'\n",
    "# password = 'FILLINYOURPASSWORD'\n",
    "username = 'hesselwinsemius'\n",
    "password = 'vH$!VO47LL#Y'\n",
    "\n",
    "freq = '36' # 36 GhZ following Brakenridge 2007 (WRR M/C ratio paper)\n",
    "HV = 'H' # Horizontal polarisation\n",
    "AD = 'A' # Ascending (also do descending!)\n",
    "\n",
    "start_date = datetime.datetime(2002, 6, 1)\n",
    "end_date = datetime.datetime(2002, 6, 5)  # only a few days to make a demonstration\n",
    "# end_date = datetime.datetime(2011, 12, 31)  # use this end date to cover the whole AMSR-E period.\n",
    "step = datetime.timedelta(days=1)  # M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need some information on geographical coverage. What bounding box would you like to retrieve? We need a projection method from latlon to the projection of the dataset itself to accomodate a proper geographical slicing. In the nsidc.py lib, a number of projection routines is provided to accomodate moving from one projection to the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the proj4string belonging to the grids you are downloading. You can find these in a downloaded sample \n",
    "# under the variable/attribute crs.proj4text. Here we simply copy-paste that.\n",
    "proj4str = '+proj=cea +lat_0=0 +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m'\n",
    "points_interest = [(2.4406387000000223, 6.893251099999999),\n",
    "                   (2.2999999999999545, 7.099999999999999),\n",
    "                   (2.4851462999999967, 8.0335995),\n",
    "                   (2.0499999999999545, 7.566667)\n",
    "]\n",
    "\n",
    "# In this example, we are interested in parts of West-Africa, so we provide a bounding box covering that in the format\n",
    "# [(xmin, ymin), (xmax, ymax)]\n",
    "bounds = [(-5., 0.),\n",
    "          (10., 15.),\n",
    "         ]\n",
    "\n",
    "# we define a projection object for lat-lon WGS84 (EPSG code 4326)\n",
    "proj_out = pyproj.Proj(init='epsg:4326')\n",
    "# we define a projection object for the projection used in the downloaded grids.\n",
    "proj_in = pyproj.Proj(proj4str)\n",
    "\n",
    "# here we convert the coordinates in lat-lon into the coordinate system of the downloaded grids.\n",
    "bounds_xy = nsidc.proj_coords(bounds, proj_out, proj_in)\n",
    "points_xy = nsidc.proj_coords(points_interest, proj_out, proj_in)\n",
    "\n",
    "# now we're all set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all information we need. Now we just need a nice loop with some intelligence when to write a file. We write it on annual basis. You can adapt this of course. Some important things:\n",
    "- we need to be able to track the location of the download and construct a proper file name. We do this by stripping parts of the url.\n",
    "- after retrieval and massaging, we need to throw away the downloaded stuff. We also strip the folder structure so that we can delete this after we're done with a file.\n",
    "- the files are apparently not entirely CF-compliant. There is a conflicting missing_value and _FillValue number. In our example, the _FillValue represented the right missing, so we simply overwrite the missing_value attribute by the _FillValue attribute in a small function correct_miss_fill. We return a cf decoded dataset from that function. This is nice because it interprets dates and times very nicely.\n",
    "- this part takes a long time if you decide to download a large dataset!! So please first test it on only a small part of the world or a few time slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = start_date\n",
    "list_ds = []\n",
    "year = dt.year # let's store data per year\n",
    "\n",
    "while dt <= end_date:\n",
    "    url = nsidc.make_measures_url(url_template, dt, freq, HV, AD)  # make the url string to download\n",
    "    fn = url.strip('https://')  # strip https:// from the url to get the local location of the downloaded file\n",
    "    path = fn.split('/')[0]  # split fn in little pieces on / and keep only the 0-th indexed piece (main folder)\n",
    "    download = nsidc.make_measures_download(download_template, url, username, password)  # pass the url to a download string template\n",
    "    success = subprocess.call(download)  # call the download string in a command-line (use wget.exe! get it from online)\n",
    "    if success == 0:\n",
    "        print('Retrieved {:s}'.format(url))\n",
    "        # the file was successfully downloaded (if not zero, there was a problem or file is sinmply not available)\n",
    "        # read file, cut a piece and add it to our list of time steps\n",
    "        ds = nsidc.correct_miss_fill(xr.open_dataset(fn, decode_cf=False))\n",
    "        ds_sel = nsidc.select_bounds(ds, bounds_xy)\n",
    "        list_ds.append(ds_sel.load())  # load the actual data so that we can delete the original downloaded files\n",
    "        ds.close()\n",
    "        shutil.rmtree(path)  # we're done cutting part of the required grid, so throw away the originally downloaded world grid.\n",
    "    dt += step   # increase by one day to go for the next download day.\n",
    "    if (year != dt.year) or (dt > end_date):  # store results if one moves to a new year or the end date is reached\n",
    "        # concatenate the list of timesteps into a new ds\n",
    "        if len(list_ds) > 0:\n",
    "            # only store if any values were found\n",
    "            ds_year = xr.concat(list_ds, dim='time')\n",
    "            # store the current dataset into a nice netcdf file\n",
    "            fn_out = os.path.abspath(os.path.join(out_path, fn_out_template.format(freq, HV, AD, year)))\n",
    "            print('Writing output for year {:d} to {:s}'.format(year, fn_out))\n",
    "            ds_year.to_netcdf(fn_out)\n",
    "        # prepare a new dataset\n",
    "        list_ds = []  # empty list\n",
    "        year = dt.year  # update the year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we should have some data now. Let's see if we can make a plot over a geographical map to demonstrate. We plot the first time step of brightness temperature in the last saved file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(fn_out)\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines(zorder=3)\n",
    "\n",
    "# get a mesh of the coordinates in grid's projection\n",
    "xi, yi = np.meshgrid(ds.x, ds.y)\n",
    "\n",
    "# the chosen projection is not (yet) supported by caropy.crs, so we need to do the transformation ourselves for now. \n",
    "# No biggy, here we go\n",
    "loni, lati = pyproj.transform(proj_in, proj_out, xi, yi)\n",
    "\n",
    "# now drape the data on the map\n",
    "p = ax.pcolormesh(loni, lati, ds['TB'][1].values, transform=ccrs.PlateCarree())\n",
    "\n",
    "# also plot some points of interest \n",
    "ax = nsidc.plot_points(ax, points_interest, marker='x', color='k', linewidth=0., transform=ccrs.PlateCarree())\n",
    "plt.colorbar(p, label='K')\n",
    "\n",
    "# Gracefully close ds\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, now we can start playing with the data. If you have retrieved a complete dataset over multiple years, please continue with the following part. We will:\n",
    "- extract a time series of brightness temperatures over a river section\n",
    "- extract time series in the surroundings of river section.\n",
    "- apply the C/M ratio method (see Brakenridge et al., 2007, http://onlinelibrary.wiley.com/doi/10.1029/2006WR005238/full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmmm interesting, but it only contains values over the north pole I think."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
